# ğŸ¤– AI Chat Application

A full-stack, production-ready AI chat application powered by Ollama, featuring a modern React + TypeScript frontend and Express backend with conversation management, multiple model support, and real-time streaming responses.

## âœ¨ Key Features

### Core Functionality
- ğŸ’¬ **Real-time AI Chat** - Interactive conversations with local AI models via Ollama
- ğŸ“ **Conversation History** - Persistent chat history with context management
- ğŸ”„ **Multiple AI Models** - Switch between different Ollama models on-the-fly
- ğŸ“± **Responsive Design** - Mobile-first UI that works on all devices
- ğŸ¨ **Syntax Highlighting** - Code blocks with language-specific highlighting
- ğŸ“Š **Markdown Support** - Rich text rendering for AI responses

### Technical Features
- âš¡ **TypeScript** - Full type safety across frontend and backend
- ğŸ›¡ï¸ **Error Handling** - Comprehensive error management and user feedback
- ğŸ” **Environment Configuration** - Secure configuration management
- ğŸ“¦ **RESTful API** - Clean API design with proper status codes
- ğŸ§ª **Development Tools** - Hot reload, linting, and debugging support
- ğŸ“ˆ **Logging** - Structured logging for monitoring and debugging

---

## ğŸ—ï¸ Architecture

```
fullstack-ollama-tutorial/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js              â† Express API server
â”‚   â”œâ”€â”€ package.json           â† Backend dependencies
â”‚   â”œâ”€â”€ .env                   â† Environment variables
â”‚   â””â”€â”€ .env.example           â† Environment template
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx            â† Main React component
â”‚   â”‚   â”œâ”€â”€ App.css            â† Application styles
â”‚   â”‚   â”œâ”€â”€ main.tsx           â† Application entry point
â”‚   â”‚   â”œâ”€â”€ types.ts           â† TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils.ts           â† Utility functions
â”‚   â”œâ”€â”€ index.html             â† HTML template
â”‚   â”œâ”€â”€ package.json           â† Frontend dependencies
â”‚   â”œâ”€â”€ tsconfig.json          â† TypeScript configuration
â”‚   â”œâ”€â”€ vite.config.js         â† Vite bundler config
â”‚   â””â”€â”€ .env                   â† Frontend environment variables
â”‚
â”œâ”€â”€ README.md                  â† Project documentation
â””â”€â”€ .gitignore                 â† Git ignore rules
```

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **React 18** - Modern UI library with hooks
- **TypeScript** - Type-safe JavaScript
- **Vite** - Lightning-fast build tool and dev server
- **CSS3** - Modern styling with animations and gradients

### Backend
- **Node.js** - JavaScript runtime
- **Express** - Minimalist web framework
- **CORS** - Cross-origin resource sharing
- **dotenv** - Environment variable management

### AI Integration
- **Ollama** - Local LLM runtime
- **REST API** - Communication with AI models

---

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following installed:
- **Node.js** v18 or higher ([Download](https://nodejs.org/))
- **Ollama** ([Download](https://ollama.ai/))
- **Git** ([Download](https://git-scm.com/))

### Verify Ollama Installation

```bash
# Check if Ollama is running
ollama list

# Pull a model (if not already installed)
ollama pull phi3:mini
# or
ollama pull llama3.2
```

---

### Installation

#### 1. Clone the Repository
```bash
git clone <your-repo-url>
cd fullstack-ollama-tutorial
```

#### 2. Setup Backend
```bash
cd backend

# Install dependencies
npm install

# Create environment file
cp .env.example .env

# Edit .env and configure your settings
# Start the server
npm start
```

#### 3. Setup Frontend
```bash
cd ../frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

#### 4. Access the Application
Open your browser and navigate to:
```
http://localhost:5173
```

---

## ğŸ“¡ API Endpoints

### Health Check
```http
GET /api/health
```
Returns server status and configuration.

### List Available Models
```http
GET /api/models
```
Returns all available Ollama models.

### Send Chat Message
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Hello, AI!",
  "model": "phi3:mini",
  "conversationId": "optional-id"
}
```

### Get Conversation History
```http
GET /api/conversations/:id
```

### Clear Conversation
```http
DELETE /api/conversations/:id
```

---

## ğŸ¯ Use Cases & Applications

This project demonstrates skills relevant to:

- **Full-Stack Development** - Complete MERN-style architecture
- **AI Integration** - Working with LLMs and AI APIs
- **TypeScript Development** - Type-safe application development
- **RESTful API Design** - Clean, scalable API architecture
- **Modern Frontend** - React hooks, state management, responsive design
- **DevOps Basics** - Environment configuration, deployment readiness

---

## ğŸš€ Deployment

### Backend Deployment
The backend can be deployed to:
- **Heroku** - Easy Node.js deployment
- **DigitalOcean** - VPS with Ollama support
- **AWS EC2** - Full control with GPU options for Ollama
- **Railway** - Modern platform for backend services

### Frontend Deployment
The frontend can be deployed to:
- **Vercel** - Optimized for Vite/React
- **Netlify** - Static site hosting with CI/CD
- **GitHub Pages** - Free static hosting
- **Cloudflare Pages** - Fast global CDN

---

## ğŸ”§ Configuration

### Environment Variables

**Backend (.env)**
```env
PORT=5000
OLLAMA_URL=http://127.0.0.1:11434
DEFAULT_MODEL=phi3:mini
NODE_ENV=development
```

**Frontend**
Update API URL in the fetch calls or create a .env file:
```env
VITE_API_URL=http://localhost:5000
```

---

## ğŸ› Troubleshooting

### Common Issues

**âŒ "Cannot connect to backend"**
- Ensure backend is running on port 5000
- Check CORS configuration
- Verify `http://localhost:5000/api/health`

**âŒ "Ollama connection failed"**
- Check if Ollama is running: `ollama list`
- Verify Ollama is on port 11434
- Try: `ollama serve`

**âŒ "Model not found"**
- Pull the model: `ollama pull phi3:mini`
- Check available models: `ollama list`

**âŒ Port already in use**
- Kill the process: `taskkill /PID <pid> /F`
- Or change the port in configuration

---

## ğŸ“š Learning Resources

- [React Documentation](https://react.dev/)
- [TypeScript Handbook](https://www.typescriptlang.org/docs/)
- [Express.js Guide](https://expressjs.com/)
- [Ollama Documentation](https://ollama.ai/docs)
- [Vite Guide](https://vitejs.dev/guide/)

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

## ğŸ‘¤ Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your Name](https://linkedin.com/in/yourprofile)
- Portfolio: [yourwebsite.com](https://yourwebsite.com)

---

## ğŸ™ Acknowledgments

- Built with [Ollama](https://ollama.ai/) - Amazing local LLM platform
- Powered by [React](https://react.dev/) and [Express](https://expressjs.com/)
- Inspired by modern AI chat applications

---

## ğŸ“ˆ Future Enhancements

- [ ] Add user authentication
- [ ] Implement conversation search
- [ ] Add file upload support
- [ ] Export conversations to PDF/Markdown
- [ ] Add voice input/output
- [ ] Implement streaming responses
- [ ] Add dark/light theme toggle
- [ ] Support for image generation models
- [ ] Multi-user support with database
- [ ] Docker containerization

// 2. Create server
const app = express();

// 3. Enable CORS (allows frontend to talk to backend)
app.use(cors());

// 4. Enable JSON parsing
app.use(express.json());

// 5. Create endpoints
app.get('/api/hello', ...);      // Test endpoint
app.post('/api/chat', ...);      // Ollama endpoint

// 6. Start server
app.listen(5000);
```

### Frontend: `App.jsx`

```javascript
// 1. State management
const [message, setMessage] = useState('');
const [response, setResponse] = useState('');

// 2. Call backend
const response = await fetch('http://localhost:5000/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ message })
});

// 3. Display response
const data = await response.json();
setResponse(data.response);
```

---

## ğŸ”§ Common Issues & Solutions

### Issue 1: "Cannot connect to backend"
**Solution:** Make sure backend server is running on port 5000
```powershell
cd C:\ApteanPay\backend
npm start
```

### Issue 2: "Failed to connect to Ollama"
**Solution:** Make sure Ollama is running
```powershell
ollama serve
```

### Issue 3: CORS errors
**Solution:** Make sure `cors` is installed and enabled in backend
```javascript
app.use(cors());
```

### Issue 4: Port already in use
**Solution:** Kill the process using the port
```powershell
# For port 5000
Get-NetTCPConnection -LocalPort 5000 | ForEach-Object { Stop-Process -Id $_.OwningProcess -Force }

# For port 5173
Get-NetTCPConnection -LocalPort 5173 | ForEach-Object { Stop-Process -Id $_.OwningProcess -Force }
```

---

## ğŸ¯ Next Steps to Learn More

1. **Add streaming responses** - Show Ollama's response word by word
2. **Add chat history** - Store previous messages
3. **Add different models** - Let user choose which Ollama model to use
4. **Add error handling** - Better error messages
5. **Add loading animations** - Better UX

---

## ğŸ“š Key Concepts

### What is CORS?
**CORS** (Cross-Origin Resource Sharing) allows your frontend (port 5173) to make requests to your backend (port 5000). Without CORS, the browser blocks these requests for security.

### What is Express?
**Express** is a minimal web framework for Node.js. It helps you create APIs easily.

### What is Vite?
**Vite** is a fast build tool for modern web projects. It's faster than Create React App.

### What is Ollama API?
**Ollama** provides a local REST API to interact with AI models. The main endpoint is:
- `POST http://localhost:11434/api/generate` - Generate text

---

## ğŸ‰ Congratulations!

You now understand:
- âœ… How React frontend makes HTTP requests
- âœ… How Express backend creates API endpoints
- âœ… How to connect to Ollama from Node.js
- âœ… How data flows through the full stack

Happy coding! ğŸš€
