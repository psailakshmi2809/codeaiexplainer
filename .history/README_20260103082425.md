# ğŸ¤– AI Chat Application

A full-stack, production-ready AI chat application powered by Ollama, featuring a modern React + TypeScript frontend and Express backend with conversation management, multiple model support, and real-time streaming responses.

## âœ¨ Key Features

### Core Functionality
- ğŸ’¬ **Real-time AI Chat** - Interactive conversations with local AI models via Ollama
- ğŸ“ **Conversation History** - Persistent chat history with context management
- ğŸ”„ **Multiple AI Models** - Switch between different Ollama models on-the-fly
- ğŸ“± **Responsive Design** - Mobile-first UI that works on all devices
- ğŸ¨ **Syntax Highlighting** - Code blocks with language-specific highlighting
- ğŸ“Š **Markdown Support** - Rich text rendering for AI responses

### Technical Features
- âš¡ **TypeScript** - Full type safety across frontend and backend
- ğŸ›¡ï¸ **Error Handling** - Comprehensive error management and user feedback
- ğŸ” **Environment Configuration** - Secure configuration management
- ğŸ“¦ **RESTful API** - Clean API design with proper status codes
- ğŸ§ª **Development Tools** - Hot reload, linting, and debugging support
- ğŸ“ˆ **Logging** - Structured logging for monitoring and debugging

---

## ğŸ—ï¸ Architecture

```
fullstack-ollama-tutorial/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js              â† Express API server
â”‚   â”œâ”€â”€ package.json           â† Backend dependencies
â”‚   â”œâ”€â”€ .env                   â† Environment variables
â”‚   â””â”€â”€ .env.example           â† Environment template
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx            â† Main React component
â”‚   â”‚   â”œâ”€â”€ App.css            â† Application styles
â”‚   â”‚   â”œâ”€â”€ main.tsx           â† Application entry point
â”‚   â”‚   â”œâ”€â”€ types.ts           â† TypeScript type definitions
â”‚   â”‚   â””â”€â”€ utils.ts           â† Utility functions
â”‚   â”œâ”€â”€ index.html             â† HTML template
â”‚   â”œâ”€â”€ package.json           â† Frontend dependencies
â”‚   â”œâ”€â”€ tsconfig.json          â† TypeScript configuration
â”‚   â”œâ”€â”€ vite.config.js         â† Vite bundler config
â”‚   â””â”€â”€ .env                   â† Frontend environment variables
â”‚
â”œâ”€â”€ README.md                  â† Project documentation
â””â”€â”€ .gitignore                 â† Git ignore rules
```

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **React 18** - Modern UI library with hooks
- **TypeScript** - Type-safe JavaScript
- **Vite** - Lightning-fast build tool and dev server
- **CSS3** - Modern styling with animations and gradients

### Backend
- **Node.js** - JavaScript runtime
- **Express** - Minimalist web framework
- **CORS** - Cross-origin resource sharing
- **dotenv** - Environment variable management

### AI Integration
- **Ollama** - Local LLM runtime
- **REST API** - Communication with AI models

---

## ğŸš€ Getting Started

### Prerequisites

Ensure you have the following installed:
- **Node.js** v18 or higher ([Download](https://nodejs.org/))
- **Ollama** ([Download](https://ollama.ai/))
- **Git** ([Download](https://git-scm.com/))

### Verify Ollama Installation

```bash
# Check if Ollama is running
ollama list

# Pull a model (if not already installed)
ollama pull phi3:mini
# or
ollama pull llama3.2
```

---

### Step 1: Install Backend Dependencies

```powershell
cd C:\ApteanPay\backend
npm install
```

This installs:
- **express** - Web framework for Node.js
- **cors** - Allows frontend to communicate with backend

---

### Step 2: Install Frontend Dependencies

```powershell
cd C:\ApteanPay\frontend
npm install
```

This installs:
- **react** - UI library
- **react-dom** - React renderer
- **vite** - Fast build tool
- **@vitejs/plugin-react** - React plugin for Vite

---

### Step 3: Start Ollama (if not running)

```powershell
ollama serve
```

Keep this terminal open. Ollama will run on **port 11434**.

---

### Step 4: Start Backend Server

Open a **new terminal**:

```powershell
cd C:\ApteanPay\backend
npm start
```

You should see:
```
âœ… Backend server is running!
ğŸ“ URL: http://localhost:5000
```

Keep this terminal open. Backend runs on **port 5000**.

---

### Step 5: Start Frontend

Open **another new terminal**:

```powershell
cd C:\ApteanPay\frontend
npm run dev
```

You should see:
```
VITE ready in XXX ms
Local: http://localhost:5173/
```

Frontend runs on **port 5173**.

---

## ğŸ§ª Test the Application

1. Open your browser: `http://localhost:5173`
2. Click **"Test Backend"** button
   - This sends a simple request to check if backend is running
3. Type a message in the text area
4. Click **"Send to Ollama"**
   - This sends your message through the backend to Ollama

---

## ğŸ”„ How Data Flows

```
User types message
    â†“
React Frontend (port 5173)
    â†“ [HTTP POST request]
Express Backend (port 5000)
    â†“ [HTTP POST request]
Ollama API (port 11434)
    â†“ [AI generates response]
Ollama sends response back
    â†“
Backend receives response
    â†“ [HTTP response]
Frontend displays response
    â†“
User sees AI response
```

---

## ğŸ“ Understanding Each File

### Backend: `server.js`

```javascript
// 1. Import packages
const express = require('express');
const cors = require('cors');

// 2. Create server
const app = express();

// 3. Enable CORS (allows frontend to talk to backend)
app.use(cors());

// 4. Enable JSON parsing
app.use(express.json());

// 5. Create endpoints
app.get('/api/hello', ...);      // Test endpoint
app.post('/api/chat', ...);      // Ollama endpoint

// 6. Start server
app.listen(5000);
```

### Frontend: `App.jsx`

```javascript
// 1. State management
const [message, setMessage] = useState('');
const [response, setResponse] = useState('');

// 2. Call backend
const response = await fetch('http://localhost:5000/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ message })
});

// 3. Display response
const data = await response.json();
setResponse(data.response);
```

---

## ğŸ”§ Common Issues & Solutions

### Issue 1: "Cannot connect to backend"
**Solution:** Make sure backend server is running on port 5000
```powershell
cd C:\ApteanPay\backend
npm start
```

### Issue 2: "Failed to connect to Ollama"
**Solution:** Make sure Ollama is running
```powershell
ollama serve
```

### Issue 3: CORS errors
**Solution:** Make sure `cors` is installed and enabled in backend
```javascript
app.use(cors());
```

### Issue 4: Port already in use
**Solution:** Kill the process using the port
```powershell
# For port 5000
Get-NetTCPConnection -LocalPort 5000 | ForEach-Object { Stop-Process -Id $_.OwningProcess -Force }

# For port 5173
Get-NetTCPConnection -LocalPort 5173 | ForEach-Object { Stop-Process -Id $_.OwningProcess -Force }
```

---

## ğŸ¯ Next Steps to Learn More

1. **Add streaming responses** - Show Ollama's response word by word
2. **Add chat history** - Store previous messages
3. **Add different models** - Let user choose which Ollama model to use
4. **Add error handling** - Better error messages
5. **Add loading animations** - Better UX

---

## ğŸ“š Key Concepts

### What is CORS?
**CORS** (Cross-Origin Resource Sharing) allows your frontend (port 5173) to make requests to your backend (port 5000). Without CORS, the browser blocks these requests for security.

### What is Express?
**Express** is a minimal web framework for Node.js. It helps you create APIs easily.

### What is Vite?
**Vite** is a fast build tool for modern web projects. It's faster than Create React App.

### What is Ollama API?
**Ollama** provides a local REST API to interact with AI models. The main endpoint is:
- `POST http://localhost:11434/api/generate` - Generate text

---

## ğŸ‰ Congratulations!

You now understand:
- âœ… How React frontend makes HTTP requests
- âœ… How Express backend creates API endpoints
- âœ… How to connect to Ollama from Node.js
- âœ… How data flows through the full stack

Happy coding! ğŸš€
